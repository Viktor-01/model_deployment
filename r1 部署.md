---
date created: 2025年2月13日，星期四，晚上18:14:03
date modified: 2025年2月20日，星期四，晚上19:05:52
---
# 硬件配置
## GPU
4台8卡3090（24G）
Compute Capability 8.6
node内GPU之间通信方式：PCIe
## CPU
Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz
(intel二代U，不支持AMX指令集)
## 内存
377G

# sglang
## 启动脚本
ref: [example-serving-with-two-h2008-nodes-and-docker](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-two-h2008-nodes-and-docker)
```
docker run --gpus all \
    --shm-size 32g \
    --network=host \
    -e GLOO_SOCKET_IFNAME=bond0 \
    -v ~/workspace:/home/hanxianlin/workspace \
    -v /share/deepseek:/share/deepseek \
    --name sglang_multinode2 \
    -it \
    --rm \
    --env "HF_TOKEN=$HF_TOKEN" \
    --ipc=host \
    docker.1ms.run/lmsysorg/sglang:latest \
    python3 -m sglang.launch_server \
    --model-path /share/deepseek/DeepSeek-R1 \
    --tp  32 \
    --dist-init-addr 214.10.10.1:20000 \
    --nnodes 4 \
    --node-rank 0 \
    --trust-remote-code 
```

### 注意事项
#### 不同节点需设置好node-rank的序号
#### 如果遇到gloo通信报错：
```log
Gloo connectFullMesh failed with [../third_party/gloo/gloo/transport/tcp/pair.cc:144] no error
```
需在启动脚本里设置环境变量
`-e GLOO_SOCKET_IFNAME=bond0 \`

## 量化推理：使用deepseek官方的FP8量化
### tp=32
报错：ValueError: Weight output_partition_size = 576 is not divisible by weight quantization block_n = 128

官方文档里的示例[example-serving-with-four-a1008-nodes](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-four-a1008-nodes)，配置是`4*8*80(A100)`,使用BF16的进度进行推理时，tp=32是可以的
而当我们使用的是`4*8*24G(3090)` 做fp8推理时候，config.json规定的`quantization_config.weight_block_size` 是 `[128, 128]` ，intermediate_size=18432, 因此output_partition_size / tp_size = 576 不能被量化后的区块尺寸128整除，所以tp_size不能为32，除非不使用config里的量化方法。
![[Pasted image 20250213203112.png]]
ref:
[[Bug] deepseek-R1 671b can not set tensor_parallel_size=32 · Issue #3345 · sgl-project/sglang](https://github.com/sgl-project/sglang/issues/3345#issuecomment-2641812423)
[sglang/benchmark/deepseek_v3 at main · sgl-project/sglang](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#troubleshooting)

### tp=16
报错：OOM
需要1.3TB的显存，而当前显存`4*8*24G(3090)`为768G。
![[1739448504715.png]]



## 使用非量化的bf16推理
删除config.json里的量化配置后，OOM
```json
  // "quantization_config": {
  //   "activation_scheme": "dynamic",
  //   "fmt": "e4m3",
  //   "quant_method": "fp8",
  //   "weight_block_size": [
  //     128,
  //     128
  //   ]
  // },
```
![[/Pasted image 20250213181420.png]]


```
ollama show --modelfile deepseek-r1:671b-q4_K_M | sed -e 's/^FROM.*/FROM deepseek-r1:671b-q4_K_M /' > Modelfile  
  
echo "PARAMETER num_gpu 20" >> Modelfile

ollama run deepseek-r1:671b-q4_K_M

```



# ollama

## deepseek-r1:671b-q4_K_M
### 8卡
使用ollama, 在一台8 * 3090（24G）内存377G的服务器上部署
报错OOM：
	error loading model: unable to allocate CUDA0 buffer 
解决：
	手动设置offload到GPU的模型层数为16：在模型的ModelFile文件内写入参数，设置 PARAMETER num_gpu 16
输出速度：1.54token/s
### 4卡
4卡gpu需要设置卸载层数为8，否则会OOM
加载ollama很久，需要20min
![[1739782720663_d 1.png]]
推理速度：1.23 tokens/s

## ollama目前不支持多机推理

# llama.cpp

## 配置环境
```
## 安装brew
apt update -y
apt-get install -y build-essential procps curl file git
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> ~/.profile
eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
brew --version

## 编译
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp
brew install cmake
mkdir build-rpc-cuda
cd build-rpc-cuda
cmake .. -DGGML_CUDA=ON -DGGML_RPC=ON
cmake --build . --config Release

```


## 单机部署：
### 2.22bit量化版：DeepSeek-R1-UD-IQ2_XXS
[unsloth/DeepSeek-R1-GGUF at main](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS)
输出速度：1.47token/s
### 4bit量化版：DeepSeek-R1-Q4_K_M
具体速度没统计，也非常慢大约 1 token/s
## 多机部署：
### 部署模型：4bit量化版-DeepSeek-R1-Q4_K_M
### 配置容器
```bash
# 在4个node上都配置容器
docker  run -dt --gpus all --name llamacpp_from_torch \
    -v /home/hanxianlin:/home/hanxianlin \
    -w /home/hanxianlin/workspace \
    -v /usr/local/cuda:/usr/local/cuda \
    -e PATH=/usr/local/cuda/bin:$PATH \
    -e LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH \
    --network=host \
    llamacpp:latest \
    /bin/bash \
    > /home/hanxianlin/workspace/llamacpp_from_torch.log 2>&1

docker exec -it llamacpp_from_torch /bin/bash
```

### 启动模型
#### 每个副节点只启动一个rpc服务
```bash
# 各个副节点执行

docker exec -it llamacpp_from_torch /bin/bash
cd /home/hanxianlin/workspace/llama.cpp/build-rpc-cuda
bin/rpc-server -p 50052

# 主节点上执行
docker exec -it llamacpp_from_torch /bin/bash

cd /home/hanxianlin/workspace/llama.cpp/build-rpc-cuda

bin/llama-cli -m /home/hanxianlin/.cache/modelscope/hub/bartowski/DeepSeek-R1-GGUF_R1-Q4_K_M_merge/DeepSeek-R1-Q4_K_M-merged.gguf  -p "你好，你是谁？写一篇AI发展的综述" -n 15 --rpc 214.10.10.2:50052,214.10.10.3:50052,214.10.10.10:50052 -ngl 30
```
参数含义：-ngl, -n都是指模型卸载到GPU的层数

##### 副节点执行情况：
在三个副节点上执行`bin/rpc-server -p 50052`时，虽然显示发现了8张卡，但实际各个节点的0号显卡被用于模型推理：
![[Pasted image 20250219165825.png]]

##### 主节点执行情况：
![[Pasted image 20250218173524.png]]

###### 推理速度 0.24 tokens/s

##### 推理时各个节点GPU运行情况：
node1用了8张，node2,3,10只用了第0号卡

node1
![[Pasted image 20250218135834.png]]


node2:
![[Pasted image 20250218135800.png]]

node3: 
![[Pasted image 20250218135626.png]]

node10：![[Pasted image 20250218135546.png]]




#### node里每个卡设置一个rpc-server
##### 副节点
在节点执行依次8个rpc命令（对应8个rpc后端）的shell脚本：
![[start_llama_cpp_multinode 2.sh]]
##### 主节点
```bash 
docker exec -it llamacpp_from_torch /bin/bash

cd /home/hanxianlin/workspace/llama.cpp/build-rpc-cuda

bin/llama-cli -m /home/hanxianlin/.cache/modelscope/hub/bartowski/DeepSeek-R1-GGUF_R1-Q4_K_M_merge/DeepSeek-R1-Q4_K_M-merged.gguf \
 -p "你好，你是谁？写一篇AI发展的综述" \
 -n 15 \
 --rpc \
214.10.10.2:50053,\
214.10.10.2:50054,\
214.10.10.2:50055,\
214.10.10.2:50056,\
214.10.10.2:50057,\
214.10.10.2:50058 \
-ngl 15
```

##### 3台服务器
共使用`3*8`张显卡
报错：llama.cpp/ggml/src/ggml-backend.cpp:1455: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed
原因：超过16个rpc服务就会报错
![[Pasted image 20250218183127.png]]

##### 2台服务器
启动的程序本身占了一个rpc进程，因此最多使用15张卡。实测使用15张卡时依然会报错，最后使用了14张卡成功
加载需要大约8min
![[Pasted image 20250218184301.png]]

###### 推理速度0.69 token/s
![[Pasted image 20250218192738.png]]


### llama.cpp多机部署总结：
模型加载：
目前加载模型的时候不是从各个rpc的本地加载，而是从主节点通过网络加载到各个rpc后端，很慢[Feature Request: RPC offloading using a local model copy · Issue #10095 · ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/issues/10095)
每次加载r1-q4的模型需要大约半小时![[Pasted image 20250218113730.png]]
模型推理：
GPU的算力很少使用，主要依靠CPU推理。


无论是模型加载、还是推理，llama.cpp用于多机部署的 [rpc](https://github.com/ggml-org/llama.cpp/tree/master/examples/rpc?__from__=talkingdev) 框架都很慢，目前（2025.02）仍处于实验阶段，而且本身不是为了优化推理速度，更多是为了加载更大的模型而设计的。


# vllm
## 环境配置

```
## 主节点
bash run_cluster.sh \
    docker.1ms.run/vllm/vllm-openai \
    214.10.10.1 \
    --head \
    /home/hanxianlin/.cache/huggingface \
    -e VLLM_HOST_IP=214.10.10.1

## 其他节点
bash run_cluster.sh \
    docker.1ms.run/vllm/vllm-openai \
    214.10.10.1 \
    --worker \
    /home/hanxianlin/.cache/huggingface \
    -e VLLM_HOST_IP=$(hostname -I | awk '{print $1}')

docker exec -it node /bin/bash
ray status
```

## q4 gguf量化
### 启动命令：
```
# 在主节点执行

vllm serve  \
	/share/deepseek/DeepSeek-R1_q4_k_m_merged/DeepSeek-R1-Q4_K_M-merged.gguf \
	--tokenizer /share/deepseek/DeepSeek-R1
	--tensor-parallel-size 8 \
	--pipeline-parallel-size 4 \
	--trust-remote-code
```
### 报错：
![[bfb206c1202f52d7987aa587ae1afdc.png]]
VLLM对于GGUF的量化依赖于transformers库的支持，而当前其并不支持deepseek2的架构 [GGUF and interaction with Transformers](https://huggingface.co/docs/transformers/gguf)


## q4 awq量化

```
# 在主节点执行
	docker exec -it node /bin/bash

vllm serve  /share/deepseek/DeepSeek-R1_671B_awq/ \
--tokenizer /share/deepseek/DeepSeek-R1 \
--tensor-parallel-size 8 \
--pipeline-parallel-size 4 \
--trust-remote-code \
--dtype float16 \
--gpu_memory_utilization 0.90 \
--max_model_len 4096 \
--quantization awq_marlin \
--kv-cache-dtype fp8 \
--max_num_seqs 1
```
### 参数设置注意事项
实测，max_num_seqs 为1，--max_model_len 4096的情况下，gpu_memory_utilization不能 >= 0.93，否则会OOM

### 部署过程
#### 模型加载
最初加载r1-q4的awq模型时，速度很慢，需要半个多小时，之后再加载就很快了

#### 推理速度
大约11~12tokens/s
![[2931c9e881158cd34e038addc8a5d09.png]]

#### 生成质量
在回答问题的时候有思维链，效果主观感觉还不错，还有待更多任务上的评测。
##### 至此，终于在4\*8\*3090的机器上部署了r1-671b-q4


