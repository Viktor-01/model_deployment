---
date created: 2025年2月19日，星期三，晚上18:00:36
date modified: 2025年2月20日，星期四，晚上18:53:16
---

# 和主线不太相关的尝试：
## gguf转为safetensor
因为VLLM不支持gguf的deepseek，因此想转为safetensor再推理，这里用的脚本[purinnohito/gguf_to_safetensors: Script to convert from GGUF format to safetensors](https://github.com/purinnohito/gguf_to_safetensors)是转为fp16精度的，这个脚本是基于llama.cpp中的方法的[llama.cpp/gguf-py/gguf/quants.py at master · ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/gguf/quants.py#L67)

但我需要的应该是转为int4精度的，关于量化我不太了解，搜索有发现网上已经有用awq的量化方法转换为int4精度的例子了，而vllm支持awq量化，[cognitivecomputations/DeepSeek-R1-AWQ · Hugging Face](https://huggingface.co/cognitivecomputations/DeepSeek-R1-AWQ)，因此就直接使用了。

### 转换格式总是被Kill
第一次被KILL
![[工作/model_deployment/附件/Pasted image 20250218140212.png]]


第二次
![[工作/model_deployment/附件/Pasted image 20250218135302.png]]


第三次
![[工作/model_deployment/附件/Pasted image 20250218145412.png]]